//
// Created by baitianyu on 7/25/24.
//
#include <google/protobuf/descriptor.h>
#include <google/protobuf/message.h>
#include "net/rpc/rpc_channel.h"
#include "net/rpc/rpc_controller.h"
#include "net/coder/tinypb/tinypb_protocol.h"
#include "common/msg_id_util.h"
#include "common/log.h"
#include "common/error_code.h"

namespace rocket {

    RPCChannel::RPCChannel(NetAddr::net_addr_sptr_t_ peer_addr) : m_peer_addr(peer_addr) {
        m_client = std::make_shared<TCPClient>(m_peer_addr);
    }

    RPCChannel::~RPCChannel() {
        DEBUGLOG("~RPCChannel");
    }

    void RPCChannel::Init(RPCChannel::google_rpc_controller_sptr_t_ controller,
                          RPCChannel::google_message_sptr_t_ request,
                          RPCChannel::google_message_sptr_t_ response, RPCChannel::google_closure_sptr_t_ done) {
        if (m_is_init) {
            return;
        }
        m_controller = controller;
        m_request = request;
        m_response = response;
        m_closure = done;
        m_is_init = true;
    }

    // 这里的CallMethod和Service里面的CallMethod(在dispatcher中)基本上差不多
    void rocket::RPCChannel::CallMethod(const google::protobuf::MethodDescriptor *method,
                                        google::protobuf::RpcController *controller,
                                        const google::protobuf::Message *request, google::protobuf::Message *response,
                                        google::protobuf::Closure *done) {
        // 保存一下当前channel的智能指针，保证回调函数调用时候channel存在，也就间接保证了回调函数调用
        // 时候可以调用google_rpc_controller_sptr_t_和google_message_sptr_t_等内容
        rpc_channel_sptr_t_ this_channel = shared_from_this();
        // 创建request并把request message放里面的pb data中
        auto req_protocol = std::make_shared<TinyPBProtocol>();
        auto rpc_controller = dynamic_cast<RPCController *>(controller);
        if (rpc_controller == nullptr) {
            ERRORLOG("failed callmethod, RpcController convert error");
            return;
        }
        if (rpc_controller->GetMSGID().empty()) {
            req_protocol->m_msg_id = MSGIDUtil::GenerateMSGID();
            rpc_controller->SetMsgId(req_protocol->m_msg_id);
        } else {
            req_protocol->m_msg_id = rpc_controller->GetMSGID();
        }
        req_protocol->m_method_name = method->full_name();
        INFOLOG("%s | call method name [%s]", req_protocol->m_msg_id.c_str(), req_protocol->m_method_name.c_str());
        // 判断是否init
        if (!m_is_init) {
            std::string err_info = "RpcChannel not call init()";
            rpc_controller->SetError(ERROR_RPC_CHANNEL_INIT, err_info);
            ERRORLOG("%s | %s, RpcChannel not init ", req_protocol->m_msg_id.c_str(), err_info.c_str());
            return;
        }
        // request的序列化，把request message序列化放到pb data里面
        if (!request->SerializeToString(&(req_protocol->m_pb_data))) {
            std::string err_info = "failed to serialize";
            rpc_controller->SetError(ERROR_FAILED_SERIALIZE, err_info); // 设置controller的error
            ERRORLOG("%s | %s, origin request [%s] ", req_protocol->m_msg_id.c_str(), err_info.c_str(),
                     request->ShortDebugString().c_str());
            return;
        }

        // 在这里注册超时回调函数，超过此时间报告rpc调用超时
        // 报告超时不需要进行重复，在回调函数中打印错误信息
        // 这里应该是在接收到回复包后从event loop中删除该定时任务，防止调用成功后仍然触发
        // 但是调用成功后应该就立即退出了，所以应该不删除也是可以的

        // important
        // 1. this channel被复制捕获到lambda表达式中，然后放到timer event中，如果该timer event没有被调用的话，那么则this channel
        //    始终持有引用计数，无法析构。应该捕获其外部引用，而不是复制，这样就不会再复制一遍this channel了，在test rpc client中
        //    调用channel.reset的时候就直接析构了，包括这里面的放到timer event中这个，因为timer event中和外面存的实际上是同一个，并不会
        //    造成引用计数+1，导致无法正常析构。
        // 2. 同理，当rpc调用超时的话，connect方法里面的各种lambda表达式都包含着一些this_channel的
        //    引用计数，导致外面无法正常析构。所以下面都应该捕获引用，保证全局只有一个rpc channel的智能指针对象
        // 3. 虽然在这个例子中test rpc client会直接退出，不输出析构，这里是不影响的。
        // 但是会不会出现this channel在外面已经析构了，然后回调函数再调用的情况，应该会有吧，所以还是应该在这里都保存一份this channel的智能指针？
        // 在lambda里面？
        m_timeout_timer_event_info = std::make_shared<TimerEventInfo>(rpc_controller->GetTimeout(),
                                                                      false, [&this_channel, rpc_controller]() {
                    INFOLOG("%s | call rpc timeout", rpc_controller->GetMSGID().c_str());
                    // finished标志结束了调用回调函数
                    if (rpc_controller->Finished()) {
                        return;
                    }
                    rpc_controller->StartCancel(); // 取消任务
                    rpc_controller->SetError(ERROR_RPC_CALL_TIMEOUT,
                                             "rpc call timeout " + std::to_string(rpc_controller->GetTimeout()));
                    if (this_channel->GetClosure()) {
                        this_channel->GetClosure()->Run();
                        rpc_controller->SetFinished(true); // 结束调用回调函数
                    }
                });
        // 添加到event loop里面
        // rpc channel的event loop负责监听是否连接成功，还有负责超时timer event，一会整理一下各个模块用到的event loop
        this_channel->GetClient()->getEventLoop()->addTimerEvent(m_timeout_timer_event_info);

        // 在client中进行connect，write和read操作
        // 这里如果client的connect返回错误了需要直接return，所以需要在client的connect中来
        // 设置connect的error code来判断是否连接成功，不能只打印一个error
        m_client->connect([req_protocol, &this_channel]() {
            // 在这里设置连接失败，并将失败信息放到rpc controller中进行返回
            auto rpc_controller = dynamic_cast<RPCController *>(this_channel->GetController());
            if (this_channel->GetClient()->getConnectErrorCode() != 0) {
                rpc_controller->SetError(this_channel->GetClient()->getConnectErrorCode(),
                                         this_channel->GetClient()->getConnectErrorInfo());
                ERRORLOG("%s | connect error, error code[%d], error info[%s], peer addr[%s]",
                         req_protocol->m_msg_id.c_str(), rpc_controller->GetErrorCode(),
                         rpc_controller->GetErrorInfo().c_str(),
                         this_channel->GetClient()->getPeerAddr()->toString().c_str());
                return;
            }
            // 写入
            this_channel->GetClient()->writeMessage(req_protocol, [req_protocol, &this_channel, rpc_controller](
                    AbstractProtocol::abstract_pro_sptr_t_ msg) {
                INFOLOG("%s | success send rpc request. call method name [%s], peer addr [%s], local addr[%s]",
                        req_protocol->m_msg_id.c_str(), req_protocol->m_method_name.c_str(),
                        this_channel->GetClient()->getPeerAddr()->toString().c_str(),
                        this_channel->GetClient()->getLocalAddr()->toString().c_str());
                // 读取
                this_channel->GetClient()->readMessage(req_protocol->m_msg_id, [&this_channel, rpc_controller](
                        AbstractProtocol::abstract_pro_sptr_t_ msg) {
                    auto rsp_protocol = std::dynamic_pointer_cast<TinyPBProtocol>(msg);
                    // 反序列化失败
                    if (!(this_channel->GetResponse()->ParseFromString(rsp_protocol->m_pb_data))) {
                        ERRORLOG("%s | serialize error", rsp_protocol->m_msg_id.c_str());
                        rpc_controller->SetError(ERROR_FAILED_SERIALIZE, "serialize error");
                        return;
                    }
                    // 协议里面的err code是否为错误
                    if (rsp_protocol->m_err_code != 0) {
                        ERRORLOG("%s | call rpc method [%s] failed, error code [%d], error info [%s]",
                                 rsp_protocol->m_msg_id.c_str(), rsp_protocol->m_method_name.c_str(),
                                 rsp_protocol->m_err_code, rsp_protocol->m_err_info.c_str());
                        rpc_controller->SetError(rsp_protocol->m_err_code, rsp_protocol->m_err_info);
                        return;
                    }
                    INFOLOG("%s | success get rpc response, call method name [%s], peer addr [%s], local addr[%s], response [%s]",
                            rsp_protocol->m_msg_id.c_str(), rsp_protocol->m_method_name.c_str(),
                            this_channel->GetClient()->getPeerAddr()->toString().c_str(),
                            this_channel->GetClient()->getLocalAddr()->toString().c_str(),
                            this_channel->GetResponse()->ShortDebugString().c_str());
                    if (this_channel->GetClosure()) {
                        this_channel->GetClosure()->Run();
                    }
                    // 释放this channel的指针并设置为nullptr，上面需要添加mutable才可以进行修改，因为lambda默认不允许修改其捕获的外部变量
                    // 如果捕获的引用的话就无所谓，因为捕获的是引用，不是复制一遍后的this_channel
                    // this_channel.reset();
                    // 但是认为this_channel应该在test rpc client中进行管理，即在那里面进行创建和reset，因为在那里无论返回正确还是错误都要reset
                    // 不应该在这里直接reset，应该由哪里创建，哪里删除
                    // 同理m_timeout_timer_event_info中也不应该去reset，应该由创建方去reset
                });
            });
        });
    }

    // get方法不会增加引用计数，只有复制shared ptr的时候才会增加引用计数
    google::protobuf::RpcController *RPCChannel::GetController() {
        return m_controller.get();
    }

    google::protobuf::Message *RPCChannel::GetRequest() {
        return m_request.get();
    }

    google::protobuf::Message *RPCChannel::GetResponse() {
        return m_response.get();
    }

    google::protobuf::Closure *RPCChannel::GetClosure() {
        return m_closure.get();
    }

    TCPClient *RPCChannel::GetClient() {
        return m_client.get();
    }

}



